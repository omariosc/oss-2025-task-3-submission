@misc{oquab_dinov2_2024,
    title = {{DINOv2}: {Learning} {Robust} {Visual} {Features} without {Supervision}},
    shorttitle = {{DINOv2}},
    url = {http://arxiv.org/abs/2304.07193},
    doi = {10.48550/arXiv.2304.07193},
    abstract = {The recent breakthroughs in natural language processing for model pretraining on large quantities of data have opened the way for similar foundation models in computer vision. These models could greatly simplify the use of images in any system by producing all-purpose visual features, i.e., features that work across image distributions and tasks without finetuning. This work shows that existing pretraining methods, especially self-supervised methods, can produce such features if trained on enough curated data from diverse sources. We revisit existing approaches and combine different techniques to scale our pretraining in terms of data and model size. Most of the technical contributions aim at accelerating and stabilizing the training at scale. In terms of data, we propose an automatic pipeline to build a dedicated, diverse, and curated image dataset instead of uncurated data, as typically done in the self-supervised literature. In terms of models, we train a ViT model (Dosovitskiy et al., 2020) with 1B parameters and distill it into a series of smaller models that surpass the best available all-purpose features, OpenCLIP (Ilharco et al., 2021) on most of the benchmarks at image and pixel levels.},
    urldate = {2025-09-17},
    publisher = {arXiv},
    author = {Oquab, Maxime and Darcet, Timothée and Moutakanni, Théo and Vo, Huy and Szafraniec, Marc and Khalidov, Vasil and Fernandez, Pierre and Haziza, Daniel and Massa, Francisco and El-Nouby, Alaaeldin and Assran, Mahmoud and Ballas, Nicolas and Galuba, Wojciech and Howes, Russell and Huang, Po-Yao and Li, Shang-Wen and Misra, Ishan and Rabbat, Michael and Sharma, Vasu and Synnaeve, Gabriel and Xu, Hu and Jegou, Hervé and Mairal, Julien and Labatut, Patrick and Joulin, Armand and Bojanowski, Piotr},
    month = feb,
    year = {2024},
    note = {arXiv:2304.07193 [cs]},
    keywords = {Computer Science - Computer Vision and Pattern Recognition},
}
@misc{meinhardt_trackformer_2022,
    title = {{TrackFormer}: {Multi}-{Object} {Tracking} with {Transformers}},
    shorttitle = {{TrackFormer}},
    url = {http://arxiv.org/abs/2101.02702},
    doi = {10.48550/arXiv.2101.02702},
    abstract = {The challenging task of multi-object tracking (MOT) requires simultaneous reasoning about track initialization, identity, and spatio-temporal trajectories. We formulate this task as a frame-to-frame set prediction problem and introduce TrackFormer, an end-to-end trainable MOT approach based on an encoder-decoder Transformer architecture. Our model achieves data association between frames via attention by evolving a set of track predictions through a video sequence. The Transformer decoder initializes new tracks from static object queries and autoregressively follows existing tracks in space and time with the conceptually new and identity preserving track queries. Both query types benefit from self- and encoder-decoder attention on global frame-level features, thereby omitting any additional graph optimization or modeling of motion and/or appearance. TrackFormer introduces a new tracking-by-attention paradigm and while simple in its design is able to achieve state-of-the-art performance on the task of multi-object tracking (MOT17 and MOT20) and segmentation (MOTS20). The code is available at https://github.com/timmeinhardt/trackformer .},
    urldate = {2025-09-17},
    publisher = {arXiv},
    author = {Meinhardt, Tim and Kirillov, Alexander and Leal-Taixe, Laura and Feichtenhofer, Christoph},
    month = apr,
    year = {2022},
    note = {arXiv:2101.02702 [cs]},
    keywords = {Computer Science - Computer Vision and Pattern Recognition},
}
@misc{ultralytics_yolov8_nodate,
    title = {{YOLOv8}},
    url = {https://docs.ultralytics.com/models/yolov8},
    abstract = {Discover Ultralytics YOLOv8, an advancement in real-time object detection, optimizing performance with an array of pre-trained models for diverse tasks.},
    language = {en},
    urldate = {2025-04-05},
    author = {Ultralytics},
}
@misc{yaseen_what_2024,
    title = {What is {YOLOv8}: {An} {In}-{Depth} {Exploration} of the {Internal} {Features} of the {Next}-{Generation} {Object} {Detector}},
    shorttitle = {What is {YOLOv8}},
    url = {http://arxiv.org/abs/2408.15857},
    doi = {10.48550/arXiv.2408.15857},
    abstract = {This study presents a detailed analysis of the YOLOv8 object detection model, focusing on its architecture, training techniques, and performance improvements over previous iterations like YOLOv5. Key innovations, including the CSPNet backbone for enhanced feature extraction, the FPN+PAN neck for superior multi-scale object detection, and the transition to an anchor-free approach, are thoroughly examined. The paper reviews YOLOv8's performance across benchmarks like Microsoft COCO and Roboflow 100, highlighting its high accuracy and real-time capabilities across diverse hardware platforms. Additionally, the study explores YOLOv8's developer-friendly enhancements, such as its unified Python package and CLI, which streamline model training and deployment. Overall, this research positions YOLOv8 as a state-of-the-art solution in the evolving object detection field.},
    urldate = {2025-04-07},
    publisher = {arXiv},
    author = {Yaseen, Muhammad},
    month = aug,
    year = {2024},
    note = {arXiv:2408.15857 [cs]
version: 1},
    keywords = {Computer Science - Computer Vision and Pattern Recognition},
}
@misc{zhang_bytetrack_2022,
    title = {{ByteTrack}: {Multi}-{Object} {Tracking} by {Associating} {Every} {Detection} {Box}},
    shorttitle = {{ByteTrack}},
    url = {http://arxiv.org/abs/2110.06864},
    doi = {10.48550/arXiv.2110.06864},
    abstract = {Multi-object tracking (MOT) aims at estimating bounding boxes and identities of objects in videos. Most methods obtain identities by associating detection boxes whose scores are higher than a threshold. The objects with low detection scores, e.g. occluded objects, are simply thrown away, which brings non-negligible true object missing and fragmented trajectories. To solve this problem, we present a simple, effective and generic association method, tracking by associating almost every detection box instead of only the high score ones. For the low score detection boxes, we utilize their similarities with tracklets to recover true objects and filter out the background detections. When applied to 9 different state-of-the-art trackers, our method achieves consistent improvement on IDF1 score ranging from 1 to 10 points. To put forwards the state-of-the-art performance of MOT, we design a simple and strong tracker, named ByteTrack. For the first time, we achieve 80.3 MOTA, 77.3 IDF1 and 63.1 HOTA on the test set of MOT17 with 30 FPS running speed on a single V100 GPU. ByteTrack also achieves state-of-the-art performance on MOT20, HiEve and BDD100K tracking benchmarks. The source code, pre-trained models with deploy versions and tutorials of applying to other trackers are released at https://github.com/ifzhang/ByteTrack.},
    urldate = {2025-01-25},
    publisher = {arXiv},
    author = {Zhang, Yifu and Sun, Peize and Jiang, Yi and Yu, Dongdong and Weng, Fucheng and Yuan, Zehuan and Luo, Ping and Liu, Wenyu and Wang, Xinggang},
    month = apr,
    year = {2022},
    note = {arXiv:2110.06864 [cs]},
    keywords = {Computer Science - Computer Vision and Pattern Recognition},
}
@misc{liu_path_2018,
    title = {Path {Aggregation} {Network} for {Instance} {Segmentation}},
    url = {http://arxiv.org/abs/1803.01534},
    doi = {10.48550/arXiv.1803.01534},
    abstract = {The way that information propagates in neural networks is of great importance. In this paper, we propose Path Aggregation Network (PANet) aiming at boosting information flow in proposal-based instance segmentation framework. Specifically, we enhance the entire feature hierarchy with accurate localization signals in lower layers by bottom-up path augmentation, which shortens the information path between lower layers and topmost feature. We present adaptive feature pooling, which links feature grid and all feature levels to make useful information in each feature level propagate directly to following proposal subnetworks. A complementary branch capturing different views for each proposal is created to further improve mask prediction. These improvements are simple to implement, with subtle extra computational overhead. Our PANet reaches the 1st place in the COCO 2017 Challenge Instance Segmentation task and the 2nd place in Object Detection task without large-batch training. It is also state-of-the-art on MVD and Cityscapes. Code is available at https://github.com/ShuLiu1993/PANet},
    urldate = {2025-09-17},
    publisher = {arXiv},
    author = {Liu, Shu and Qi, Lu and Qin, Haifang and Shi, Jianping and Jia, Jiaya},
    month = sep,
    year = {2018},
    note = {arXiv:1803.01534 [cs]},
    keywords = {Computer Science - Computer Vision and Pattern Recognition},
}
@misc{lin_feature_2017,
    title = {Feature {Pyramid} {Networks} for {Object} {Detection}},
    url = {http://arxiv.org/abs/1612.03144},
    doi = {10.48550/arXiv.1612.03144},
    abstract = {Feature pyramids are a basic component in recognition systems for detecting objects at different scales. But recent deep learning object detectors have avoided pyramid representations, in part because they are compute and memory intensive. In this paper, we exploit the inherent multi-scale, pyramidal hierarchy of deep convolutional networks to construct feature pyramids with marginal extra cost. A top-down architecture with lateral connections is developed for building high-level semantic feature maps at all scales. This architecture, called a Feature Pyramid Network (FPN), shows significant improvement as a generic feature extractor in several applications. Using FPN in a basic Faster R-CNN system, our method achieves state-of-the-art single-model results on the COCO detection benchmark without bells and whistles, surpassing all existing single-model entries including those from the COCO 2016 challenge winners. In addition, our method can run at 5 FPS on a GPU and thus is a practical and accurate solution to multi-scale object detection. Code will be made publicly available.},
    urldate = {2025-09-17},
    publisher = {arXiv},
    author = {Lin, Tsung-Yi and Dollár, Piotr and Girshick, Ross and He, Kaiming and Hariharan, Bharath and Belongie, Serge},
    month = apr,
    year = {2017},
    note = {arXiv:1612.03144 [cs]},
    keywords = {Computer Science - Computer Vision and Pattern Recognition},
}